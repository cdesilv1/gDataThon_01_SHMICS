{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.0 64-bit",
   "display_name": "Python 3.8.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "9c2441ec92c423bccdc265a9f8eaefe1e886e230cb3c9107344ff4cb68d32709"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     C:\\Users\\Trevor\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "import tweepy as tw\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, RegexpTokenizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction import stop_words\n",
    "stopwords = stop_words.ENGLISH_STOP_WORDS\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "class TextCleaner():\n",
    "    '''\n",
    "    This class instantiates an object with attributes of text preprocessing dictionaries and \n",
    "    a method for applying this to a list of text. \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Removed groups: \n",
    "            r\"[!?$%()*+,-./:;<=>\\^_`{|}~]\"\n",
    "        '''\n",
    "        self.re_substitution_groups = [r'^RT', r'^rt', r'http\\S+', r'&amp; ', r'^[@#]\\w+']\n",
    "        self.text_abbrevs = { 'lol': 'laughing out loud', 'bfn': 'bye for now', 'cuz': 'because',\n",
    "                            'afk': 'away from keyboard', 'nvm': 'never mind', 'iirc': 'if i recall correctly',\n",
    "                            'ttyl': 'talk to you later', 'imho': 'in my honest opinion', 'brb': 'be right back',\n",
    "                            \"fyi\": \"for your information\" }\n",
    "        self.grammar_abbrevs = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                             \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                             \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                             \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                             \"mustn't\":\"must not\", \"'s\":\"s\"}\n",
    "\n",
    "\n",
    "    def clean_tweets(self, df_tweet_text, last_clean_step=6):\n",
    "        '''\n",
    "        INPUT: df_tweet_text <string>\n",
    "        This function will clean the text of tweets, with ability to very the last step of cleaning.\n",
    "        order:\n",
    "        1. lowercase\n",
    "        2. change txt abbreviations\n",
    "        3. change grammar abbreviation\n",
    "        4. remove punctuation\n",
    "        5. remove special (utf-8) characters\n",
    "        6. remove stop words\n",
    "        Run on one tweet at a time, for example:\n",
    "        cleaner = TextCleaner()\n",
    "        df['clean_tweets'] = df['full_text'].apply(lambda x: cleaner.clean_tweets(x, 5))\n",
    "        '''\n",
    "        df_tweet_text_sw = str(df_tweet_text)\n",
    "\n",
    "        if last_clean_step == 0:\n",
    "            clean_text = df_tweet_text_sw\n",
    "\n",
    "        elif last_clean_step == 1:\n",
    "            clean_text = df_tweet_text_sw.lower()\n",
    "\n",
    "        elif last_clean_step == 2:\n",
    "            lower = df_tweet_text_sw.lower()\n",
    "            clean_text = ' '.join([self.text_abbrevs.get(elem, elem) for elem in lower.split()])\n",
    "        \n",
    "        elif last_clean_step == 3:\n",
    "            lower = df_tweet_text_sw.lower()\n",
    "            without_text_abbrevs = ' '.join([self.text_abbrevs.get(elem, elem) for elem in lower.split()])\n",
    "            clean_text = ' '.join([self.grammar_abbrevs.get(elem, elem) for elem in without_text_abbrevs.split()])\n",
    "        \n",
    "        elif last_clean_step == 4:\n",
    "            lower = df_tweet_text_sw.lower()\n",
    "            without_text_abbrevs = ' '.join([self.text_abbrevs.get(elem, elem) for elem in lower.split()])\n",
    "            without_grammar_abbrevs = ' '.join([self.grammar_abbrevs.get(elem, elem) for elem in without_text_abbrevs.split()])\n",
    "            \n",
    "            joined_re_groups = '|'.join([group for group in self.re_substitution_groups])\n",
    "            clean_text = ' '.join([re.sub(joined_re_groups,' ',word) for word in without_grammar_abbrevs.split()])\n",
    "        \n",
    "        elif last_clean_step == 5:\n",
    "            lower = df_tweet_text_sw.lower()\n",
    "            without_text_abbrevs = ' '.join([self.text_abbrevs.get(elem, elem) for elem in lower.split()])\n",
    "            without_grammar_abbrevs = ' '.join([self.grammar_abbrevs.get(elem, elem) for elem in without_text_abbrevs.split()])\n",
    "            \n",
    "            joined_re_groups = '|'.join([group for group in self.re_substitution_groups])\n",
    "            without_re_groups = ' '.join([re.sub(joined_re_groups,' ',word) for word in without_grammar_abbrevs.split()])\n",
    "\n",
    "            clean_text = re.sub(r'\\W',' ',without_re_groups)\n",
    "\n",
    "        elif last_clean_step == 6:\n",
    "            lower = df_tweet_text_sw.lower()\n",
    "            without_text_abbrevs = ' '.join([self.text_abbrevs.get(elem, elem) for elem in lower.split()])\n",
    "            without_grammar_abbrevs = ' '.join([self.grammar_abbrevs.get(elem, elem) for elem in without_text_abbrevs.split()])\n",
    "            \n",
    "            joined_re_groups = '|'.join([group for group in self.re_substitution_groups])\n",
    "            without_re_groups = ' '.join([re.sub(joined_re_groups,' ',word) for word in without_grammar_abbrevs.split()])\n",
    "\n",
    "            without_nontext = re.sub(r'\\W',' ',without_re_groups)\n",
    "\n",
    "            clean_text = ' '.join([word for word in without_nontext.split() if word not in stopwords])\n",
    "        \n",
    "        # words_greater_than_two_char = ' '.join([word for word in clean_text.split() if len(word) >= 3])\n",
    "\n",
    "        one_space_separated_tweet = ' '.join([word for word in clean_text.split()])\n",
    "\n",
    "        return one_space_separated_tweet\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=my_tokenizer,\n",
    "    stop_words='english',\n",
    "    max_features=5000)\n",
    "\n",
    "mat = vectorizer.fit_transform(X).toarray()\n",
    "X_full=pd.DataFrame(data=mat)\n",
    "\n",
    "def cross_val(X,y,over_=True):\n",
    "\n",
    "    rec_scores=[]\n",
    "    prec_scores=[]\n",
    "    f1_scores=[]\n",
    "\n",
    "    for urgh in range(1):\n",
    "        kf = KFold(n_splits=5,shuffle=True)\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            if over_==True:\n",
    "                over = SMOTE(sampling_strategy='not majority')\n",
    "                X_train,y_train=over.fit_resample(X_train,y_train)\n",
    "                X_train=pd.DataFrame(columns=X_test.columns,data=X_train)\n",
    "\n",
    "\n",
    "            model.fit(X_train,y_train)\n",
    "\n",
    "            preds=model.predict(X_test)\n",
    "\n",
    "            prec_scores.append(precision_score(y_test,preds))\n",
    "            rec_scores.append(recall_score(y_test,preds))\n",
    "            f1_scores.append(f1_score(y_test,preds))\n",
    "            print (precision_score(y_test,preds),recall_score(y_test,preds))\n",
    "    prec_res=np.mean(prec_scores)\n",
    "    rec_res=np.mean(rec_scores)\n",
    "    f1_res=np.mean(f1_scores)\n",
    "    print ('Precision = {:.2f}, Recall = {:.2f}, F1 score = {:.2f}'.format(prec_res,rec_res,f1_res))\n",
    "    return (np.mean(prec_scores),np.mean(rec_scores),np.mean(f1_scores))"
   ]
  },
  {
   "source": [
    "Loading the labelled hashtags"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annots = json.load(open(\"C:/Users/Trevor/Downloads/first_half_annotated.json\"))\n",
    "hashes=list(annots['#presidentialcandidate'].values())\n",
    "values=list(annots['0'].values())\n",
    "annots1={hashes[i]:values[i] for i in range(len(hashes))}\n",
    "\n",
    "annots2 = json.load(open(\"C:/Users/Trevor/Downloads/second_half_hashtags_annotated.json\"))\n",
    "\n",
    "annots1_df=pd.DataFrame.from_dict(annots1, orient='index')\n",
    "annots2_df=pd.DataFrame.from_dict(annots2, orient='index')\n",
    "annots=pd.concat([annots1_df,annots2_df])\n",
    "pro_trump=list(annots[annots[0]==1].index)\n",
    "pro_biden=list(annots[annots[0]==-1].index)\n",
    "neutral=list(annots[annots[0]==0].index)\n",
    "\n",
    "def clean_hash(x):\n",
    "    x=['#' + i.replace('#','') for i in x]\n",
    "    return x\n",
    "\n",
    "pro_trump=clean_hash(pro_trump)\n",
    "pro_biden=clean_hash(pro_biden)\n",
    "neutral=clean_hash(neutral)\n",
    "\n"
   ]
  },
  {
   "source": [
    "Scraping tweets containing the hashtags"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "\n",
    "consumer_key= ''\n",
    "consumer_secret= ''\n",
    "access_token= ''\n",
    "access_token_secret= ''\n",
    "\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "import random\n",
    "tmp_lst=pro_trump.copy() + pro_biden.copy()\n",
    "random.shuffle(tmp_lst)\n",
    "\n",
    "for j in range(len(tmp_lst)):\n",
    "    try:\n",
    "        search = tw.Cursor(api.search,\n",
    "                    q=tmp_lst[j],\n",
    "                    lang=\"en\",\n",
    "                    tweet_mode=\"extended\").items(100)\n",
    "        lst=[i for i in search]\n",
    "        tweets=[]\n",
    "        for i in lst:\n",
    "            try:\n",
    "                tweets.append(i.retweeted_status.full_text)\n",
    "            \n",
    "            except:\n",
    "                tweets.append(i.full_text)\n",
    "        all_tweets+=tweets\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    pd.Series(all_tweets).to_csv('scraped_tweets.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def my_tokenizer(doc):\n",
    "    \"\"\"Tokenizes document using RegExpTokenizer\n",
    "    Args:\n",
    "        doc: string\n",
    "    Returns:\n",
    "        list: tokenized words\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # tokenizer= TweetTokenizer()\n",
    "    article_tokens = tokenizer.tokenize(doc.lower())\n",
    "    return article_tokens\n",
    "\n",
    "cleaner = TextCleaner()"
   ]
  },
  {
   "source": [
    "Combining the scraped tweets with the original dataset, and dropping duplicates"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=pd.read_csv('C:/Users/Trevor/Downloads/scraped_tweets.csv')\n",
    "df=df.drop_duplicates()\n",
    "df.reset_index(inplace=True)\n",
    "df.drop(columns='index',inplace=True)\n",
    "\n",
    "df['0']=df['0'].apply(lambda x: ' '.join(x.replace('\\n',' ').split()))\n",
    "\n",
    "df2=pd.read_json('C:/Users/Trevor/Downloads/concatenated_abridged.jsonl.gz',compression='gzip',lines=True)\n",
    "more_tweets=[]\n",
    "\n",
    "for i in range(df2.shape[0]):\n",
    "    try:\n",
    "        more_tweets.append(df2.iloc[i]['retweeted_status']['full_text'])\n",
    "    except:\n",
    "        more_tweets.append(df2.iloc[i]['full_text'])\n",
    "df2['tweet']=more_tweets\n",
    "df2['tweet']=df2['tweet'].apply(lambda x: ' '.join(x.replace('\\n',' ').split()))\n",
    "df_tweets=pd.concat([df2['tweet'],df['0']])\n",
    "\n",
    "df_tweets=df_tweets.drop_duplicates()\n",
    "\n"
   ]
  },
  {
   "source": [
    "Scoring each tweet (biden/trump) using the hashtags"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=[]\n",
    "for i in list(df_tweets):\n",
    "    trump_score=0\n",
    "    neutral_score=0\n",
    "    biden_score=0\n",
    "    for j in pro_trump:\n",
    "        if j in i:\n",
    "            trump_score+=1\n",
    "    for k in neutral:\n",
    "        if k in i:\n",
    "            neutral_score+=1\n",
    "    for l in pro_biden:\n",
    "        if l in i:\n",
    "            biden_score+=1\n",
    "    scores.append((trump_score,neutral_score,biden_score))"
   ]
  },
  {
   "source": [
    "Training the Biden classifier model\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores=pd.DataFrame(columns=['pro_trump','neutral','pro_biden'],data=np.vstack(scores))\n",
    "\n",
    "all_scores['biden_score']=all_scores['pro_biden']-all_scores['pro_trump']\n",
    "all_scores['trump_score']=all_scores['pro_trump']-all_scores['pro_biden']\n",
    "all_scores['trump']=all_scores['trump_score'].apply(lambda x: 1 if x>0 else 0)\n",
    "all_scores['biden']=all_scores['biden_score'].apply(lambda x: 1 if x>0 else 0)\n",
    "\n",
    "X=df_tweets.iloc[all_scores[(all_scores['pro_trump']>0) | (all_scores['pro_biden']>0)].index]\n",
    "\n",
    "X=X.reset_index()[0]\n",
    "\n",
    "X=X.apply(lambda x: x.split('http')[0])\n",
    "\n",
    "y=all_scores[(all_scores['pro_trump']>0) | (all_scores['pro_biden']>0)]['biden']\n",
    "y=pd.DataFrame(y).reset_index()['biden']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Trevor\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X=X.apply(lambda x: ' '.join([i.lower() for i in x.split() if '#' not in i]))\n",
    "\n",
    "X=X.apply(lambda x: re.sub(r'[^\\w\\s]','',x))\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "X=X.apply(lambda x: ' '.join([i.lower() for i in x.split() if i not in stop_words]))\n",
    "\n",
    "y=y[~(X=='')]\n",
    "X=X[~(X=='')]\n",
    "\n",
    "X.to_csv('biden_data.csv',index=False)\n",
    "y.to_csv('biden_target.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.read_csv('biden_data.csv')['0']\n",
    "y=pd.read_csv('biden_target.csv').iloc[:,0]\n",
    "\n",
    "vectorizer.fit(X)\n",
    "\n",
    "mat = vectorizer.transform(X).toarray()\n",
    "X_full=pd.DataFrame(data=mat)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model=MultinomialNB()\n",
    "model.fit(X_full,y)\n",
    "\n",
    "import pickle\n",
    "\n",
    "filename = 'NB_biden.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "source": [
    "Training the Trump classifier model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_tweets.iloc[all_scores[(all_scores['pro_trump']>0) | (all_scores['pro_biden']>0)].index]\n",
    "\n",
    "X=X.reset_index()[0]\n",
    "\n",
    "X=X.apply(lambda x: x.split('http')[0])\n",
    "\n",
    "y=all_scores[(all_scores['pro_trump']>0) | (all_scores['pro_biden']>0)]['trump']\n",
    "y=pd.DataFrame(y).reset_index()['trump']\n",
    "\n",
    "X=X.apply(lambda x: ' '.join([i.lower() for i in x.split() if '#' not in i]))\n",
    "\n",
    "X=X.apply(lambda x: re.sub(r'[^\\w\\s]','',x))\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "X=X.apply(lambda x: ' '.join([i.lower() for i in x.split() if i not in stop_words]))\n",
    "\n",
    "y=y[~(X=='')]\n",
    "X=X[~(X=='')]\n",
    "\n",
    "X.to_csv('trump_data.csv',index=False)\n",
    "y.to_csv('trump_target.csv',index=False)\n",
    "\n",
    "X=pd.read_csv('trump_data.csv')['0']\n",
    "y=pd.read_csv('trump_target.csv').iloc[:,0]\n",
    "\n",
    "vectorizer.fit(X)\n",
    "\n",
    "mat = vectorizer.transform(X).toarray()\n",
    "X_full=pd.DataFrame(data=mat)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model=MultinomialNB()\n",
    "model.fit(X_full,y)\n",
    "\n",
    "import pickle\n",
    "\n",
    "filename = 'NB_trump.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  }
 ]
}